This is the code base for CVPR2022 paper "**Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model**"

# Prepare data
Download dataset according to [LVIS](https://www.lvisdataset.org/), [VOC](http://host.robots.ox.ac.uk/pascal/VOC/), [COCO](https://cocodataset.org/#home) and [Objects365](https://www.objects365.org/overview.html). Precomputed proposals generated by RPN trained on only base classes can be downloaded [here](https://cloud.tsinghua.edu.cn/d/0abae4deda2144b4b562/). It is recommended to download and extract the dataset somewhere outside the project directory and symlink the dataset root to data as below.
```
├── mmdet
├── tools
├── configs
├── data
├── ├── lvis_v1
├── ├── ├──annotations
├── ├── ├──train2017
├── ├── ├──val2017
├── ├── ├──proposals
│   ├── coco
│   │   ├── annotations
│   │   ├── train2017
│   │   ├── val2017
│   ├── VOCdevkit
│   │   ├── VOC2007
│   │   ├── VOC2012
│   ├── objects365
│   │   ├── annotations
│   │   ├── train
│   │   ├── val

```
All models use the backbone pretrained with [SoCo](https://github.com/hologerry/SoCo) which can be downloaded [here](https://cloud.tsinghua.edu.cn/f/9faa344e688e41619e92/?dl=1). Put the pretrained backbone under the project root dir.
# Main Results
| Model                  | Lr Schd   | AP<sup>bb</sup><sub>r</sub> | AP<sup>bb</sup><sub>c</sub> | AP<sup>bb</sup><sub>f</sub> | AP<sup>bb</sup> | AP<sup>mk</sup><sub>r</sub> | AP<sup>mk</sup><sub>c</sub>| AP<sup>mk</sup><sub>f</sub> | AP<sup>mk</sup> | Config | Prompt | Model |
| ---------------------- | --------- | ---- | ---- | ---- | ---- | ------- | ---- | ---- | ---- | ------ | ------ | ----- |
| ViLD*                   | 20 epochs | 17.4 | 27.5 | 31.9 | 27.5 | 16.8 | 25.6 | 28.5 | 25.2 | [config](https://github.com/dyabel/detpro/blob/main/configs/lvis/detpro_ens_20e.py) |  [prompt](https://cloud.tsinghua.edu.cn/f/3f9017c3e217496ebc25/?dl=1) | [model](https://cloud.tsinghua.edu.cn/f/d57e11e2ebf24d509218/?dl=1)      |
| DetPro (Mask R-CNN)    | 20 epochs | 20.8 | 27.8 | 32.4 | 28.4 | 19.8 | 25.6 | 28.9 | 25.9 | [config](https://github.com/dyabel/detpro/blob/main/configs/lvis/detpro_ens_20e.py) | [prompt](https://cloud.tsinghua.edu.cn/f/0fceb9cae4c249188170/?dl=1) | [model](https://cloud.tsinghua.edu.cn/f/91cecd9ef97843339c79/?dl=1) |
| DetPro (Cascade R-CNN) | 20 epochs | 21.6 | 29.8 | 35.1 | 30.5 | 19.8 | 26.8 | 30.3 | 26.9 | [config](https://github.com/dyabel/detpro/blob/main/configs/lvis/cascade_mask_rcnn_r50_fpn_sample1e-3_mstrain_20e_lvis_v1_pretrain_ens.py) | [prompt](https://cloud.tsinghua.edu.cn/f/0fceb9cae4c249188170/?dl=1) | [model](https://cloud.tsinghua.edu.cn/f/f75712011cd342bdb49e/?dl=1) | 

In the original implementation of ViLD, the whole training process takes up to 180,000 iterations with batchsize of 256, approximately 460 epochs, which is unaffordable. We re-implement ViLD (denoted as ViLD*) with backbone pretrained using [SoCo](https://github.com/hologerry/SoCo). Our re-implementation version achieves comparable AP compared with the original implementation, while reducing the training epochs from 460 to 20.
# Installation
This repo is built on [mmdetection](https://github.com/open-mmlab/mmdetection), [CLIP](https://github.com/openai/CLIP.git) and [CoOP](https://github.com/kaiyangzhou/coop)

```shell
pip install -r requirements/build.txt
pip install -e .
pip install git+https://github.com/openai/CLIP.git
pip uninstall pycocotools -y
pip uninstall mmpycocotools -y
pip install mmpycocotools
pip install git+https://github.com/lvis-dataset/lvis-api.git
pip install mmcv-full==1.2.5 -f https://download.openmmlab.com/mmcv/dist/cu110/torch1.7.0/index.html
```
# Get Started
## Quick reproduction of the main results
```
./tools/dist_test.sh <config> <model> <gpu_num> --eval bbox segm --cfg-options model.roi_head.prompt_path=<prompt> model.roi_head.load_feature=False 
```
## Prepare data for DetPro training
see [prepare.sh](https://github.com/dyabel/detpro/blob/main/prepare.sh)
## Train DetPro
see [detpro.sh](https://github.com/dyabel/detpro/blob/main/detpro.sh)
## Train ViLD with DetPro (Mask R-CNN)
see [vild_detpro.sh](https://github.com/dyabel/detpro/blob/main/vild_detpro.sh)
## Tranfer experiments
see [transer.sh](https://github.com/dyabel/detpro/blob/main/transfer.sh)
# Citation
